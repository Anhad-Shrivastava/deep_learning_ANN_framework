{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201404e2",
   "metadata": {},
   "source": [
    "- h5py is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- matplotlib is a famous library to plot graphs in Python.\n",
    "- PIL and scipy are used here to test your model with your own picture at the end.\n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f185ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f571d",
   "metadata": {},
   "source": [
    "### Forward Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84517c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations: binary_step, linear_slope, sigmoid, tanh, relu, leaky_relu, parameterized_relu, elu, swish, softmax\n",
    "def binary_step(z):\n",
    "    if x<0: A = 0\n",
    "    else: A = 1\n",
    "    chache = z\n",
    "    return A, cache\n",
    "def linear_slope(z,slope):\n",
    "    A = slope*z\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def sigmoid(z):\n",
    "    A = 1/(1+ np.exp(-1*z))\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def tanh(z):\n",
    "    A = np.tanh(z)\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def relu(z):\n",
    "    A = max(0,z)\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def leaky_relu(z):\n",
    "    if z>0: A=z\n",
    "    else: A = 0.01*z\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def parameterized_relu(z,param):\n",
    "    if z>0: A=z\n",
    "    else: A = param*z\n",
    "    cache=z\n",
    "    return A, cache\n",
    "def elu(z,param):\n",
    "    if z<0: A=param*(np.exp(x)-1)\n",
    "    else: A=z\n",
    "    cache=z\n",
    "    return A, cache\n",
    "def swish(z):\n",
    "    A = z/(1+np.exp(-1*z))\n",
    "    cache = z\n",
    "    return A, cache\n",
    "def softmax(z):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    A = exps / np.sum(exps)\n",
    "    cache=z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ef842",
   "metadata": {},
   "source": [
    "### Backward Propogations for all activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc977d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations: binary_step, linear_slope, sigmoid, tanh, relu, leaky_relu, parameterized_relu, elu, swish, softmax\n",
    "def binary_step_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = dA*0\n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def linear_slope_backward(dA, cache, slope):\n",
    "    Z = cache\n",
    "    dZ = dA*slope\n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    t = np.tanh(Z)\n",
    "    dZ = 1- np.power(t,2)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, set dz to 0 \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape) \n",
    "    return dZ\n",
    "def leaky_relu_backward(dA, cache):\n",
    "    Z=cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0.01*dZ\n",
    "    assert (dZ.shape == Z.shape) \n",
    "    return dZ\n",
    "def parameterized_relu_backward(dA, cache, param):\n",
    "    Z=cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = param*dZ\n",
    "    assert (dZ.shape == Z.shape) \n",
    "    return dZ\n",
    "def elu_backward(dA, cache, param):\n",
    "    Z=cache\n",
    "    dZ=a*np.exp(Z)\n",
    "    assert (dZ.shape == Z.shape) \n",
    "    return dZ\n",
    "def swish_backward(dA, cache):\n",
    "    Z=cache\n",
    "    sig = 1/(1+np.exp(-Z))\n",
    "    swish = Z/(1+np.exp(-1*Z))\n",
    "    dZ = swish + sig*(1-swish)\n",
    "    assert (dZ.shape == Z.shape) \n",
    "    return dZ\n",
    "\n",
    "#helper function to compute derivative of softmax function\n",
    "def softmax_grad(softmax):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax.reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "#actual softmax function\n",
    "def softmax_backward(dA, cache){\n",
    "    Z = cache\n",
    "    grad  = softmax_grad(Z)\n",
    "    dZ = dA*Z    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3f7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33c852d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(1, 3)\n",
      "type:<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHRIV_~1\\AppData\\Local\\Temp/ipykernel_5364/2212317429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"type:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdZ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoftmax_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SHRIV_~1\\AppData\\Local\\Temp/ipykernel_5364/3708197489.py\u001b[0m in \u001b[0;36msoftmax_backward\u001b[1;34m(dA, cache)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "#trash code\n",
    "def softmax_backward(dA, cache):\n",
    "    Z=cache\n",
    "    dZ = np.copy(dA)\n",
    "    for i in range(dZ.shape[0]):\n",
    "        for j in range(dZ.shape[1]):\n",
    "            if i == j:\n",
    "                dZ[i,j] = dA[i][j] * (1-dA[i][j])\n",
    "            else: \n",
    "                dZ[i,j] = -dA[i][j] * dA[j][i]\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "arr = np.array([[1.,2.,3.]])\n",
    "print(\"shape:\"+str(arr.shape))\n",
    "print(\"type:\"+str(type(arr)))\n",
    "dZ=softmax_backward(arr,arr)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81ff0b",
   "metadata": {},
   "source": [
    "Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90eb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #(≈ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = ...\n",
    "        # parameters['b' + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros([layer_dims[l],1])\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c960f76",
   "metadata": {},
   "source": [
    "Linear Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e99912cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implements the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e71827",
   "metadata": {},
   "source": [
    "Linear Activation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0ec4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "    \n",
    "    #activations: binary_step, linear, sigmoid, tanh, relu, leaky_relu, parameterized_relu, elu, swish, softmax\n",
    "    if activation == \"binary_step\":\n",
    "        A, activation_cache = binary\n",
    "    \n",
    "    elif activation == \"linear_slope\":\n",
    "        A, activation_cache = linear_slope(Z)\n",
    "    \n",
    "    elif activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = tanh(Z)\n",
    "    \n",
    "    elif activation == \"relu\":        \n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"leaky_relu\":        \n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "        \n",
    "    elif activation == \"parameterized_relu\":        \n",
    "        A, activation_cache = parameterized_relu(Z)\n",
    "        \n",
    "    elif activation == \"elu\":        \n",
    "        A, activation_cache = elu(Z)\n",
    "                \n",
    "    elif activation == \"swish\":        \n",
    "        A, activation_cache = swish(Z)\n",
    "\n",
    "    elif activation == \"softmax\":        \n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "        \n",
    "    #collect cache\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83c4e1",
   "metadata": {},
   "source": [
    "L Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters,layers):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    layers -- a list of layers that need to be applied to the network\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        #(≈ 2 lines of code)\n",
    "        # A, cache = ...\n",
    "        # caches ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        A,cache = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],layers[l-1])\n",
    "        caches.append(cache)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    #(≈ 2 lines of code)\n",
    "    # AL, cache = ...\n",
    "    # caches ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    AL, cache = linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],layers[L-1])\n",
    "    caches.append(cache)\n",
    "    # YOUR CODE ENDS HERE\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72211c",
   "metadata": {},
   "source": [
    "Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865eeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    # (≈ 1 lines of code)\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    cost = (-1/m)* (np.dot(Y,np.log(AL.T)) + np.dot(1-Y,np.log(1-AL.T)))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7a15d",
   "metadata": {},
   "source": [
    "Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa06e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # dW = ...\n",
    "    # db = ... sum by the rows of dZ with keepdims=True\n",
    "    # dA_prev = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3690d",
   "metadata": {},
   "source": [
    "Linear Activation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94870873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    #retrieve cache\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    #find dZ based on layer's activation\n",
    "    if activation == \"binary_step\":\n",
    "        dZ = binary_step_backward(dA,activation_cache)\n",
    "    \n",
    "    elif activation == \"linear\":\n",
    "        dZ = linear_slope_backward(dA,activation_cache)\n",
    "    \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA,activation_cache)\n",
    "    \n",
    "    elif activation == \"relu\":        \n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        \n",
    "    elif activation == \"leaky_relu\":        \n",
    "        dZ = leaky_relu_backward(dA,activation_cache)\n",
    "        \n",
    "    elif activation == \"parameterized_relu\":        \n",
    "        dZ = parameterized_relu_backward(dA,activation_cache)\n",
    "        \n",
    "    elif activation == \"elu\":        \n",
    "        dZ = elu_backward(dA,activation_cache)\n",
    "                \n",
    "    elif activation == \"swish\":        \n",
    "        dZ = swish_backward(dA,activation_cache)\n",
    "\n",
    "    elif activation == \"softmax\":        \n",
    "        dZ = softmax_backward(dA,activation_cache)\n",
    "    \n",
    "    #use dZ to find dA_prev, dW, db\n",
    "    dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765f5d4",
   "metadata": {},
   "source": [
    "L Model Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, layers):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(approx. 5 lines)\n",
    "    # current_cache = ...\n",
    "    # dA_prev_temp, dW_temp, db_temp = ...\n",
    "    # grads[\"dA\" + str(L-1)] = ...\n",
    "    # grads[\"dW\" + str(L)] = ...\n",
    "    # grads[\"db\" + str(L)] = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp,dw_temp,db_temp = linear_activation_backward(dAL,current_cache,layers[L-1])\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dw_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" +str(l+1)],current_cache,layers[l])\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a7958",
   "metadata": {},
   "source": [
    "Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    #(≈ 2 lines of code)\n",
    "    for l in range(L):\n",
    "        # parameters[\"W\" + str(l+1)] = ...\n",
    "        # parameters[\"b\" + str(l+1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80be13a",
   "metadata": {},
   "source": [
    "L Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        #(≈ 1 line of code)\n",
    "        # AL, caches = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost.\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        cost = compute_cost(AL,Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "        # Backward propagation.\n",
    "        #(≈ 1 line of code)\n",
    "        # grads = ...    \n",
    "        # YOUR CODE STARTS HERE\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    " \n",
    "        # Update parameters.\n",
    "        #(≈ 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ff124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca456989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd09ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba8fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d78d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac543cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dedc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77252b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "clf.fit(X.T, Y.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922c8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d10778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2246ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1e7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d33cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565deab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
